# Create the First Deployment

You could get the help message of the `mdz deploy` command by running `mdz deploy -h`.

```
Deploys a new deployment directly via flags.

Usage:
  mdz deploy [flags]

Examples:
  mdz deploy --image=modelzai/llm-blomdz-560m:23.06.13
  mdz deploy --image=modelzai/llm-blomdz-560m:23.06.13 --name blomdz-560m --node-labels gpu=true,name=node-name

Flags:
      --gpu int               Number of GPUs
  -h, --help                  help for deploy
      --image string          Image to deploy
      --max-replicas int32    Maximum number of replicas (default 1)
      --min-replicas int32    Minimum number of replicas (can be 0) (default 1)
      --name string           Name of inference
  -l, --node-labels strings   Node labels
      --port int32            Port to deploy on (default 8080)

Global Flags:
      --debug        Enable debug logging
  -u, --url string   URL to use for the server (MDZ_URL) (default http://localhost:80)
```

You could deploy a model by running `mdz deploy --image <image>`. The deployment will be accessible from the outside world by default.

```
$ mdz deploy --image aikain/simplehttpserver:0.1 --name simple-server --port 80
Inference simple-server is created
$ mdz list
 NAME           ENDPOINT                                                          STATUS  INVOCATIONS  REPLICAS 
 simple-server  http://simple-server-4k2epq5lynxbaayn.192.168.71.93.modelz.live   Ready             2  1/1      
                http://192.168.71.93.modelz.live/inference/simple-server.default                                 
```

You could access the deployment by visiting the endpoint URL. It will be `http://simple-server-4k2epq5lynxbaayn.192.168.71.93.modelz.live` in this case. The endpoint could be accessed from the outside world as well if you've provided the public IP address of your server to the `mdz server start` command.

## GPU support

import { Callout } from 'nextra/components'
 
<Callout type="info" emoji="⚠️">
  This requires the Nvidia Toolkit to be installed on the host machine before you run `mdz server start`. Check out the [Nvidia Toolkit installation guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) for more information.
</Callout>

If your service requires GPUs, you can add the `--gpu 1` flag to indicate how many GPUs you need. The deployment will be scheduled to a GPU node.

If you have different type of GPUs, you can label your nodes with `mdz server label node3 gpu=true type=nvidia-a100` and deploy the serevice to the selected nodes with `mdz deploy --image llama2 --node-labels gpu=true,type=nvidia-a100`.

## Scale your deployment

You could scale your deployment by using the `mdz scale` command.

```bash
$ mdz scale simple-server --replicas 3
```

The requests will be load balanced between the replicas of your deployment.

## Debug your deployment

Sometimes you may want to debug your deployment. You could use the `mdz logs` command to get the logs of your deployment.

```bash
$ mdz logs simple-server
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:16] "GET / HTTP/1.1" 200 -
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:16] "GET / HTTP/1.1" 200 -
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:17] "GET / HTTP/1.1" 200 -
```

You could also use the `mdz exec` command to execute a command in the container of your deployment. You do not need to ssh into the server to do that.

```
$ mdz exec simple-server ps
PID   USER     TIME   COMMAND
    1 root       0:00 /usr/bin/dumb-init /bin/sh -c python3 -m http.server 80
    7 root       0:00 /bin/sh -c python3 -m http.server 80
    8 root       0:00 python3 -m http.server 80
    9 root       0:00 ps
$ mdz exec simple-server -ti bash
bash-4.4# uname -r
5.19.0-46-generic
bash-4.4# 
```

Or you could port-forward the deployment to your local machine and debug it locally.

```
$ mdz port-forward simple-server 7860
Forwarding inference simple-server to local port 7860
```
