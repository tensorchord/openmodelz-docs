# Create the First Deployment

You could get the help message of the `mdz deploy` command by running `mdz deploy -h`.

```
Deploys a new deployment directly via flags.

Usage:
  mdz deploy [flags]

Examples:
  mdz deploy --image=modelzai/llm-blomdz-560m:23.06.13
  mdz deploy --image=modelzai/llm-blomdz-560m:23.06.13 --name blomdz-560m --node-labels gpu=true,name=node-name

Flags:
      --gpu int               Number of GPUs
  -h, --help                  help for deploy
      --image string          Image to deploy
      --max-replicas int32    Maximum number of replicas (default 1)
      --min-replicas int32    Minimum number of replicas (can be 0) (default 1)
      --name string           Name of inference
  -l, --node-labels strings   Node labels
      --port int32            Port to deploy on (default 8080)

Global Flags:
      --debug        Enable debug logging
  -u, --url string   URL to use for the server (MDZ_URL) (default http://localhost:80)
```

You could deploy a model by running `mdz deploy --image <image>`. The deployment will be accessible from the outside world by default.

```
$ mdz deploy --image aikain/simplehttpserver:0.1 --name simple-server --port 80
Inference simple-server is created
$ mdz list
 NAME           ENDPOINT                                                          STATUS  INVOCATIONS  REPLICAS 
 simple-server  http://simple-server-4k2epq5lynxbaayn.192.168.71.93.modelz.live   Ready             2  1/1      
                http://192.168.71.93.modelz.live/inference/simple-server.default                                 
```

You could access the deployment by visiting the endpoint URL. It will be `http://simple-server-4k2epq5lynxbaayn.192.168.71.93.modelz.live` in this case. The endpoint could be accessed from the outside world as well if you've provided the public IP address of your server to the `mdz server start` command.

## GPU support

import { Callout } from 'nextra/components'
 
<Callout type="info" emoji="⚠️">
  This requires the Nvidia Driver to be installed on the host machine before you run `mdz server start`. Check out the [Nvidia Driver Downloads](https://www.nvidia.com/Download/index.aspx) for more information.

  If you are using the GPU VM from cloud providers, the driver should be installed already.

  For OS before Ubuntu 22.04, you might need to run the following command to make sure the GPU operator is working properly.

  ```bash
  sysctl -w fs.inotify.max_user_watches=100000 
  sysctl -w fs.inotify.max_user_instances=100000
  ```

  If you're using Debian, you might need to add a symlink before you run `mdz server start`:

  ```bash
  ln -s /sbin/ldconfig /sbin/ldconfig.real
  ```
</Callout>

If your service requires GPUs, you can add the `--gpu 1` flag to indicate how many GPUs you need. The deployment will be scheduled to a GPU node.

If you have different type of GPUs, you can label your nodes with `mdz server label node3 gpu=true type=nvidia-a100` and deploy the serevice to the selected nodes with `mdz deploy --image llama2 --node-labels gpu=true,type=nvidia-a100`.

## Scale your deployment

You could scale your deployment by using the `mdz scale` command.

```bash
$ mdz scale simple-server --replicas 3
```

The requests will be load balanced between the replicas of your deployment.

## Debug your deployment

Sometimes you may want to debug your deployment. You could use the `mdz logs` command to get the logs of your deployment.

```bash
$ mdz logs simple-server
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:16] "GET / HTTP/1.1" 200 -
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:16] "GET / HTTP/1.1" 200 -
simple-server-6756dd67ff-4bf4g: 10.42.0.1 - - [27/Jul/2023 02:32:17] "GET / HTTP/1.1" 200 -
```

You could also use the `mdz exec` command to execute a command in the container of your deployment. You do not need to ssh into the server to do that.

```
$ mdz exec simple-server ps
PID   USER     TIME   COMMAND
    1 root       0:00 /usr/bin/dumb-init /bin/sh -c python3 -m http.server 80
    7 root       0:00 /bin/sh -c python3 -m http.server 80
    8 root       0:00 python3 -m http.server 80
    9 root       0:00 ps
$ mdz exec simple-server -ti bash
bash-4.4# uname -r
5.19.0-46-generic
bash-4.4# 
```

Or you could port-forward the deployment to your local machine and debug it locally.

```
$ mdz port-forward simple-server 7860
Forwarding inference simple-server to local port 7860
```
